groups:
  - name: aerospike.rules
    rules:
    - alert: ExporterAgentDown
      expr: up{job="aerospike"} == 0
      for: 30s
      labels:
        severity: critical
      annotations:
        description: '{{ $labels.instance }} has been down for more than 30 seconds.'
        summary: Node {{ $labels.instance }} down 

    - alert: AerospikeNodeDown
      expr: aerospike_node_up{job="aerospike"} == 0
      for: 30s
      labels:
        severity: critical
      annotations:
        description: '{{ $labels.instance }} node is down.'
        summary: Node {{ $labels.instance }} down
    - alert: NamespaceStopWrites
      expr: aerospike_namespace_stop_writes{job="aerospike"} == 1
      for: 30s
      labels:
        severity: critical
      annotations:
        description: 'Used diskspace for namespace {{ $labels.ns }} in node {{ $labels.instance }} is above stop writes limit.'
        summary: Stop Writes for {{ $labels.instance }}/{{ $labels.ns }}
    - alert: NamespaceMemoryCloseToStopWrites
      expr: (aerospike_namespace_stop_writes_pct{job="aerospike"} - (100 - aerospike_namespace_memory_free_pct{job="aerospike"})) <= 10
      for: 30s
      labels:
        severity: warn
      annotations:
        description: 'Free memory for namespace {{ $labels.ns }} in node {{ $labels.instance }} is close to stop writes limit.'
        summary: Close to stop writes for {{ $labels.instance }}/{{ $labels.ns }} due to memory
    - alert: NamespaceDiskCloseToStopWrites
      expr: (aerospike_namespace_device_available_pct{job="aerospike"} - aerospike_namespace_storage_engine_min_avail_pct{job="aerospike"}) <= 10
      for: 30s
      labels:
        severity: warn
      annotations:
        description: 'device_available_pct for namespace {{ $labels.ns }} in node {{ $labels.instance }} is close to min-avail-pct (stop writes) limit.'
        summary: Close to stop writes for {{ $labels.instance }}/{{ $labels.ns }} due to device_available_pct
    - alert: NamespacePmemCloseToStopWrites
      expr: (aerospike_namespace_pmem_available_pct{job="aerospike"} - aerospike_namespace_storage_engine_min_avail_pct{job="aerospike"}) <= 10
      for: 30s
      labels:
        severity: warn
      annotations:
        description: 'pmem_available_pct for namespace {{ $labels.ns }} in node {{ $labels.instance }} is close to min-avail-pct (stop writes) limit.'
        summary: Close to stop writes for {{ $labels.instance }}/{{ $labels.ns }} due to pmem_available_pct
    - alert: ClockSkewStopWrites
      expr: aerospike_namespace_clock_skew_stop_writes{job="aerospike"} == 1
      for: 30s
      labels:
        severity: critical
      annotations:
        description: 'Clock has skewed for namespace {{ $labels.ns }} in node {{ $labels.instance }}'
        summary: Writes will be stopped by Aerospike
    - alert: ClusterSize
      expr: aerospike_node_stats_cluster_size{job="aerospike"} < 3 # user has to replace 3 here with their desired cluster size.
      for: 30s
      labels:
        severity: critical
      annotations:
        description: 'Cluster size mismatch for namespace {{ $labels.ns }} in node {{ $labels.instance }}'
        summary: Some of the node(s) has gone out of the cluster
    - alert: DeadPartitions
      expr: aerospike_namespace_dead_partitions{job="aerospike"} > 0
      for: 30s
      labels:
        severity: critical
      annotations:
        description: 'Some of the partition(s) for namespace {{ $labels.ns }} in node {{ $labels.instance }} are dead'
        summary: Will require the use of the revive command to make them available again
    - alert: HwmBreached
      expr: aerospike_namespace_hwm_breached{job="aerospike"} == 1
      for: 30s
      labels:
        severity: critical
      annotations:
        description: 'High water mark has breached for namespace {{ $labels.ns }} in node {{ $labels.instance }}'
        summary: Eviction will start to make the space available
    - alert: ActiveProxies
      expr: (increase(aerospike_namespace_client_proxy_complete{job="aerospike"}[2m]) + increase(aerospike_namespace_client_proxy_timeout{job="aerospike"}[2m]) + increase(aerospike_namespace_client_proxy_error{job="aerospike"}[2m]) + increase(aerospike_namespace_batch_sub_proxy_complete{job="aerospike"}[2m]) + increase(aerospike_namespace_batch_sub_proxy_timeout{job="aerospike"}[2m]) + increase(aerospike_namespace_batch_sub_proxy_error{job="aerospike"}[2m])) > 0
      for: 30s
      labels:
        severity: warn
      annotations:
        description: 'Active proxies detected for {{ $labels.ns }} on node {{ $labels.instance }}'
        summary: Node is doing proxy. Proxies can happen during cluster change / migrations or if there are any network issues.
    - alert: NamespaceSupervisorFallingBehind
      expr: aerospike_namespace_nsup_cycle_deleted_pct{job="aerospike"} > 1 # (Aerospike 6.3 and later) 
      for: 30s
      labels:
        severity: critical
      annotations:
        description: 'There seems some lag falling behind and/or display the length of time the most recent NSUP cycle lasted {{ $labels.ns }} in node {{ $labels.instance }}'
        summary: NSUP is falling behind and/or display the length of time the most recent NSUP cycle lasted
    - alert: NamespaceFreeMemoryCloseToStopWrites
      expr: (aerospike_namespace_stop_writes_sys_memory_pct{job="aerospike"} - scalar(100 - (aerospike_node_stats_system_free_mem_pct{job="aerospike"}))) <= 10
      for: 30s
      labels:
        severity: critical
      annotations:
        description: 'Free memory for namespace {{ $labels.ns }} in node {{ $labels.instance }} is close to stop writes limit.'
        summary: Close to stop writes for {{ $labels.instance }}/{{ $labels.ns }} due to memory
    - alert: NamespaceSetQuotaWarning
      expr: (((aerospike_sets_device_data_bytes{job="aerospike"} + aerospike_sets_memory_data_bytes{job="aerospike"}) / (aerospike_sets_stop_writes_size{job="aerospike"} != 0)) * 100) > 80
      for: 30s
      labels:
        severity: critical
      annotations:
        description: 'Nearing memory quota for {{ $labels.set }} in namespace {{ $labels.ns }} in node {{ $labels.instance }}.'
        summary: One of your nodes is at 80% of the quota you have configured on the set.
    - alert: NamespaceSetQuotaAlert
      expr: (((aerospike_sets_device_data_bytes{job="aerospike"} + aerospike_sets_memory_data_bytes{job="aerospike"}) / (aerospike_sets_stop_writes_size{job="aerospike"} != 0)) * 100) > 99
      for: 30s
      labels:
        severity: critical
      annotations:
        description: 'At or Above memory quota for {{ $labels.set }} in namespace {{ $labels.ns }} in node {{ $labels.instance }}.'
        summary: One of your nodes is at 99% of the quota you have configured on the set.

    - alert: LowDeviceAvailWarning
      expr: aerospike_namespace_device_available_pct{job="aerospike"} < 20
      for: 30s
      labels:
        severity: warn
      annotations:
        summary: "{% raw %}Device available warning for {{ $labels.instance }}/{{ $labels.ns }}{% endraw %}"
        description: "Device available has dropped below 20 %{% raw %} for namespace {{ $labels.ns }} in node {{ $labels.instance }}. May indicate that defrag is unable to keep up with the current load, and may result in stop writes if it continues to drop.{% endraw %}"
    - alert: LowDeviceAvailCritical
      expr: aerospike_namespace_device_available_pct{job="aerospike"} < 10
      for: 30s
      labels:
        severity: critical
      annotations:
        summary: "{% raw %}Device available critically low for {{ $labels.instance }}/{{ $labels.ns }}{% endraw %}"
        description: "Device available has dropped below 10 %{% raw %} for namespace {{ $labels.ns }} in node {{ $labels.instance }}. May indicate that defrag is unable to keep up with the current load, and may result in stop writes if it continues to drop.{% endraw %}"
    - alert: ClientConnectionsWarning
      expr: aerospike_node_stats_client_connections{job="aerospike"} > 100
      for: 30s
      labels:
        severity: warn
      annotations:
        summary: "Client connections warning"
        description: "Client connections are greater than 100. Connections will fail if they exceed proto-fd-max."

    - alert: ClientConnectionsCritical
      expr: aerospike_node_stats_client_connections{job="aerospike" } > 300
      for: 30s
      labels:
        severity: critical
      annotations:
        summary: "Client connections warning"
        description: "Client connections are greater than expected peak of 300."
    - alert: ClientConnectionChurn
      expr: rate(aerospike_node_stats_client_connections_opened{job="aerospike"}[1m]) > 10 or rate(aerospike_node_stats_client_connections_closed{job="aerospike"}[1m]) > 10
      for: 30s
      labels:
        severity: critical
      annotations:
        summary: "Clients are churning connections at a high rate"
        description: "Client connections are being opened or closed at a rate greater than 10/m. Connection churn can increase latency and client timeouts which in turn cause the client to open more connections."
    - alert: ClientTimeouts
      expr: rate(aerospike_namespace_client_read_timeout{job=~"aerospike"}[1m]) > 10 or rate(aerospike_namespace_client_write_timeout{job=~"aerospike"}[1m]) > 10 or rate(aerospike_namespace_client_tsvc_timeout{job=~"aerospike"}[1m]) > 10
      for: 30s
      labels:
        severity: critical
      annotations:
        summary: "Client transactions are timing out"
        description: "Client connections timing out at a rate greater than 10/s. Timeouts can occur during network issues or resource contention on the client and/or server nodes."

    - alert: ClockSkewWarning
      expr: aerospike_node_stats_cluster_clock_skew_ms{job="aerospike" } > 10
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "Cluster clock skew warning{"
        description: "Current maximum clock skew between nodes - will trigger stop writes when it exceeds 10 seconds if nsup-period is non-zero."
    - alert: ClockSkewCritical
      expr: aerospike_node_stats_cluster_clock_skew_ms{job="aerospike" } > 20
      for: 30s
      labels:
        severity: critical
      annotations:
        summary: "Cluster clock skew critical alert"
        description: "Current maximum clock skew between nodes - will trigger stop writes when it exceeds 20 seconds if nsup-period is non-zero."

    - alert: LowMemoryNamespaceWarning
      expr: aerospike_namespace_memory_free_pct{job="aerospike" } < 30
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "{% raw %}Memory available warning for {{ $labels.instance }}/{{ $labels.ns }}{% endraw %}"
        description: "Memory free has dropped below 30%{% raw %} for namespace {{ $labels.ns }} in node {{ $labels.instance }}. May indicate a need to reduce the object count or increase capacity.{% endraw %}"
    - alert: LowMemoryNamespaceCritical
      expr: aerospike_namespace_memory_free_pct{job="aerospike" } <20
      for: 30s
      labels:
        severity: critical
      annotations:
        summary: "{% raw %}Memory available critically low for {{ $labels.instance }}/{{ $labels.ns }}{% endraw %}"
        description: "Memory free has dropped below 20 %{% raw %} for namespace {{ $labels.ns }} in node {{ $labels.instance }}. May indicate a need to reduce the object count or increase capacity.{% endraw %}"
    - alert: LowMemorySystemWarning
      expr: aerospike_node_stats_system_free_mem_pct{job="aerospike" } < 30
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "{% raw %}Memory available warning for {{ $labels.instance }}{% endraw %}"
        description: "Total memory free has dropped below 30%{% raw %} for node {{ $labels.instance }}.{% endraw %}"
    - alert: LowMemorySystemCritical
      expr: aerospike_node_stats_system_free_mem_pct{job="aerospike" } < 20
      for: 30s
      labels:
        severity: critical
      annotations:
        summary: "{% raw %}Memory available critically low for {{ $labels.instance }}{% endraw %}"
        description: "Total memory free has dropped below 20%{% raw %} for node {{ $labels.instance }}.{% endraw %}"

    - alert: HeapEfficiencyWarning
      #expr: aerospike_node_stats_heap_efficiency_pct{job="aerospike" } < 60
      expr: (100 - aerospike_node_stats_system_free_mem_pct{job="aerospike" }) > 30  and aerospike_node_stats_heap_efficiency_pct{job="aerospike" } < 30
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "{% raw %}Heap efficiency warning for {{ $labels.instance }}{% endraw %}"
        description: "{% raw %}Heap efficiency for node for {{ $labels.instance }} has dropped below {% endraw %}60%."

    - alert: DeviceWriteQWarning
      expr: aerospike_namespace_storage_engine_device_write_q{job="aerospike" } > 10
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "{% raw %}Device write queue high for {{ $labels.instance }}/{{ $labels.ns }}/{{ $labels.device_index }}{% endraw %}"
        description: "Device write queue is greater than 10{%raw%} for namespace {{ $labels.ns }} on device {{ $labels.device_index }} in node {{ $labels.instance }}. May indicate underperforming storage subsystem or hotkeys.{% endraw %}"

    - alert: ShadowDeviceWriteQWarning
      expr: aerospike_namespace_storage_engine_device_shadow_write_q{job="aerospike" } > 10
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "{% raw %}Shadow device write queue high for {{ $labels.instance }}/{{ $labels.ns }}/{{ $labels.device_index }}{% endraw %}"
        description: "Shadow device write queue is greater than 10 {% raw %}for namespace {{ $labels.ns }} on device {{ $labels.device_index }} in node {{ $labels.instance }}. May indicate underperforming storage subsystem or hotkeys.{% endraw %}"

    - alert: DeviceDefragQWarning
      expr: aerospike_namespace_storage_engine_device_defrag_q{job="aerospike" }> 10
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "{% raw %}Device defrag queue high for {{ $labels.instance }}/{{ $labels.ns }}/{{ $labels.device_index }}{% endraw %}"
        description: "Device defrag queue has been above 10 for more than  '5m'{% raw %} for namespace {{ $labels.ns }} on device {{ $labels.device_index }} in node {{ $labels.instance }}. May indicate underperforming storage subsystem or hotkeys.{% endraw %}"

    - alert: RwInProgressWarning
      expr: aerospike_node_stats_rw_in_progress{job="aerospike" }> 10
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "{% raw %}Read/write queue too high for {{ $labels.instance }}/{{ $labels.ns }}/{{ $labels.device_index }}{% endraw %}"
        description: "Read/write queue is greater than 10{% raw %} for namespace {{ $labels.ns }} on device {{ $labels.device_index }} in node {{ $labels.instance }}. May indicate underperforming storage subsystem or hotkeys.{% endraw %}"

    - alert: UnavailablePartitions
      expr: aerospike_namespace_unavailable_partitions{job="aerospike" } > 0
      for: 30s
      labels:
        severity: critical
      annotations:
        summary: "Some partitions are inaccessible, and roster nodes are missing from the cluster."
        description: "{% raw %}Some partitions are not available for namespace {{ $labels.ns }} on node {{ $labels.instance }}. Check for network issues and make sure the cluster forms properly.{% endraw %}"

    - alert: ReadLatencyP95Warning
      expr: histogram_quantile(0.95, (aerospike_latencies_read_ms_bucket{job="aerospike" })) > 2
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "{% raw %}Read latency breached for {{ $labels.ns }} on {{ $labels.hostname }}{% endraw %}"
        description: "95th percentile read latency breached 2{% raw %}ms for namespace {{ $labels.ns }} on host {{ $labels.hostname }}.{% endraw %}"

    - alert: ReadLatencyP99Warning
      expr: histogram_quantile(0.99, (aerospike_latencies_read_ms_bucket{job="aerospike" })) > 4
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "{% raw %}Read latency breached for {{ $labels.ns }} on {{ $labels.hostname }}{% endraw %}"
        description: "99th percentile read latency breached 4{% raw %}ms for namespace {{ $labels.ns }} on host {{ $labels.hostname }}.{% endraw %}"
    - alert: ReadLatencyP999Warning
      expr: histogram_quantile(0.999, (aerospike_latencies_read_ms_bucket{job="aerospike" })) > 16
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "{% raw %}Read latency breached for {{ $labels.ns }} on {{ $labels.hostname }}{% endraw %}"
        description: "99.9th percentile read latency breached 16{% raw %}ms for namespace {{ $labels.ns }} on host {{ $labels.hostname }}.{% endraw %}"

    - alert: WriteLatencyP95Warning
      expr: histogram_quantile(0.95, (aerospike_latencies_write_ms_bucket{job="aerospike" })) > 4
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "{% raw %}Write latency breached for {{ $labels.ns }} on {{ $labels.hostname }}{% endraw %}"
        description: "95th percentile write latency breached 4{% raw %}ms for namespace {{ $labels.ns }} on host {{ $labels.hostname }}.{% endraw %}"
    - alert: WriteLatencyP99Warning
      expr: histogram_quantile(0.99, (aerospike_latencies_write_ms_bucket{job="aerospike" })) > 16
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "{% raw %}Write latency breached for {{ $labels.ns }} on {{ $labels.hostname }}{% endraw %}"
        description: "99th percentile write latency breached 16{% raw %}ms for namespace {{ $labels.ns }} on host {{ $labels.hostname }}.{% endraw %}"
    - alert: WriteLatencyP999Warning
      expr: histogram_quantile(0.999, (aerospike_latencies_write_ms_bucket{job="aerospike" })) > 64
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "{% raw %}Write latency breached for {{ $labels.ns }} on {{ $labels.hostname }}{% endraw %}"
        description: "99.9th percentile write latency breached 64{% raw %}ms for namespace {{ $labels.ns }} on host {{ $labels.hostname }}.{% endraw %}"

  - name: xdr.rules
    rules:
    - alert: XDRTimelag_4.9
      expr: aerospike_node_stats_xdr_timelag{job="aerospike"} > 10 # (Aerospike 4.9 and older) user can configure the seconds. Refer XDR throttling
      for: 30s
      labels:
        severity: warn
      annotations:
        description: 'There seems some lag in XDR for namespace {{ $labels.ns }} in node {{ $labels.instance }}'
        summary: Lag can be there in XDR due to network connectivity issues or errors writing at a destination cluster
    - alert: XDRTimelag
      expr: aerospike_xdr_lag{job="aerospike" } > 10
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "{% raw %}XDR lag for namespace {{ $labels.ns }} exceeding {% endraw %}10{% raw %} second(s) from node {{ $labels.instance }} to DC {{ $labels.dc }}{% endraw %}"
        description: "XDR lag may be due to network connectivity issues, inability for the source to keep up with incoming writes, or write failures at the destination."

    - alert: XDRAbandonedRecords
      expr: rate(aerospike_xdr_abandoned{job="aerospike" }[1m]) > 0
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "{% raw %}Abandoned records detected for XDR on node {{ $labels.instance }} to DC {{ $labels.dc }}{% endraw %}"
        description: "{% raw %}Records abandoned at a destination cluster may indicate a configuration mismatch for the namespace between source and destination.{% endraw %}"

    - alert: XDRRetryNoNode
      expr: rate(aerospike_xdr_retry_no_node{job="aerospike" }[1m]) > 0
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "{% raw %}XDR retries occuring on node {{ $labels.instance }} to DC {{ $labels.dc }} due to unknown master node destination{% endraw %}"
        description: "XDR cannot determine which destination node is the master."

    - alert: XDRRetryConnReset
      expr: rate(aerospike_xdr_retry_conn_reset{job="aerospike" }[1m]) > 10
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "Rate of XDR connection resets greater than 10{% raw %}/s from {{ $labels.instance }} to DC {{ $labels.dc }} {% endraw %}"
        description: "XDR retries occuring due to due to timeouts, network problems, or destination node restarts."

    - alert: XDRRetryDest
      expr: rate(aerospike_xdr_retry_dest{job="aerospike" }[1m]) > 10
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "Increase in XDR write retries is greater than 10{% raw %}/s from {{ $labels.instance }} to DC {{ $labels.dc }}{% endraw %}"
        description: "XDR retries due to errors returned by the destination node, u.e. key busy or device overload."

    - alert: XDRLatencyWarning
      expr: aerospike_xdr_latency_ms{job="aerospike" } > 10
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "XDR latency above 10{% raw %}ms from {{ $labels.instance }} to DC {{ $labels.dc }}{% endraw %}"
        description: "Network latency between XDR source and destination over the last 10 is higher than expected."

    - alert: XDRLap
      expr: aerospike_xdr_lap_us{job="aerospike" } > 10
      for: 30s
      labels:
         severity: warn
      annotations:
        summary: "XDR lap time greater than 10{% raw %} microseconds from {{ $labels.instance }} to DC {{ $labels.dc }}{% endraw %}"
        description: "The XDR processing cycle time (lap_us) is approaching the configured period-ms value."
    - alert: XDRRecoveries
      expr: increase(aerospike_xdr_recoveries{job="aerospike" }[1m]) > 0
      for:  30s
      labels:
        severity: critical
      annotations:
        summary: "{% raw %}XDR recoveries increasing on {{ $labels.instance }} to DC {{ $labels.dc }}{% endraw %}"
        description: "XDR recoveries happen during reind or may indicate that the in-memory transaction queue is full (the transaction-queue-limit may be too small)."
