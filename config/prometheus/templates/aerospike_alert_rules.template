groups:
- name: aerospike.rules
  rules:
  - alert: AerospikeExporterAgentDown
    expr: up{job="aerospike"} == 0
    for: {{ aerospike_exporter_agent_down_duration }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Aerospike Prometheus exporter job {{ $labels.instance }} down{% endraw %}"
      description: "{% raw %}{{ $labels.instance }} has been down for more than {% endraw %}{{ aerospike_exporter_agent_down_duration }}."

  - alert: AerospikeNodeDown
    expr: aerospike_node_up{job="aerospike"} == 0
    for: {{ aerospike_node_down_duration }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Node {{ $labels.instance }} down{% endraw %}"
      description: "{% raw %}{{ $labels.instance }} node is down.{% endraw %}"

{% for thresholds in alertmanager_aerospike_metric_thresholds %}
- name: aerospike_{{ thresholds.cluster }}.rules > NAMESPACE
  rules:
  - alert: NamespaceStopWrites
    expr: aerospike_namespace_stop_writes{job="aerospike", cluster_name="{{ thresholds.cluster }}"} == 1
    for: {{ thresholds.aerospike_namespace_stop_writes_duration | default(aerospike_namespace_stop_writes_duration) }}
    labels:
      severity: critical
    annotations:
      summary: "{% raw %}Stop writes for {{ $labels.instance }}/{{ $labels.ns }}{% endraw %}"
      description: "{% raw %}Used disk space for namespace {{ $labels.ns }} in node {{ $labels.instance }} is above stop writes limit. {% endraw %}. <a href='localhost:7100' target='_blank'>namespace view </a>"

  - alert: AerospikeAllFlashAverageObjectsPerSprig
    expr:  ( ((aerospike_namespace_master_objects { job="aerospike", cluster_name="{{ thresholds.cluster }}" }/4096)/aerospike_namespace_partition_tree_sprigs{ job="aerospike", cluster_name="{{ thresholds.cluster }}" } ) and ignoring (index, sindex) ((aerospike_namespace_index_type_mounts_size_limit { job="aerospike", cluster_name="{{ thresholds.cluster }}" }) or (aerospike_namespace_sindex_type_mounts_size_limit { job="aerospike", cluster_name="{{ thresholds.cluster }}" }) ))> {{ thresholds.average_all_flash_objects_per_sprig | default(average_all_flash_objects_per_sprig) }}
    for: {{ thresholds.aerospike_average_objects_breached_duration | default(aerospike_average_objects_breached_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Average Objects per sprig in {{ $labels.instance  }}/{{ $labels.ns }}{% endraw %}"
      description: "{% raw %}Average objects per sprig has been breached for namespace {{ $labels.ns }} in node {{ $labels.instance }}. {% endraw %}"

  - alert: AerospikeAverageObjectsPerSprig
    expr:  ( ((aerospike_namespace_master_objects { job="aerospike", cluster_name="{{ thresholds.cluster }}" }/4096)/aerospike_namespace_partition_tree_sprigs{ job="aerospike", cluster_name="{{ thresholds.cluster }}" } ) unless ignoring (index, sindex) ((aerospike_namespace_index_type_mounts_size_limit { job="aerospike", cluster_name="{{ thresholds.cluster }}" }) or (aerospike_namespace_sindex_type_mounts_size_limit { job="aerospike", cluster_name="{{ thresholds.cluster }}" }) ))> {{ thresholds.average_objects_per_sprig | default(average_objects_per_sprig) }}
    for: {{ thresholds.aerospike_average_objects_breached_duration | default(aerospike_average_objects_breached_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Average Objects per sprig in {{ $labels.instance  }}/{{ $labels.ns }}{% endraw %}"
      description: "{% raw %}Average objects per sprig has been breached for namespace {{ $labels.ns }} in node {{ $labels.instance }}. {% endraw %}"

  - alert: AerospikeIndexStageSizeWarn
    # Check here: https://docs.aerospike.com/reference/configuration#index-stage-size
    #  <128mb or >4gb -- send warn alert
    expr:  (aerospike_namespace_index_stage_size{job="aerospike", cluster_name="{{ thresholds.cluster }}"}>{{ thresholds.aerospike_index_stage_size_suggested_max | default(aerospike_index_stage_size_suggested_max) }})  
    for: {{ thresholds.aerospike_index_stage_size_range_duration | default(aerospike_index_stage_size_range_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Index stage size configuration is not configured according to documentation in {{ $labels.instance  }}/{{ $labels.ns }}{% endraw %}"
      description: "{% raw %}Index stage size configuration is not configured according to documentation in {{ $labels.ns }} in node {{ $labels.instance }}. {% endraw %}"

  - alert: AerospikeSIndexStageSizeWarn
    # Check here: https://docs.aerospike.com/reference/configuration#sindex-stage-size
    #  <128mb or >4gb -- send warn alert
    expr:  (aerospike_namespace_sindex_stage_size{job="aerospike", cluster_name="{{ thresholds.cluster }}"}>{{ thresholds.aerospike_sindex_stage_size_suggested_max | default(aerospike_sindex_stage_size_suggested_max) }})  
    for: {{ thresholds.aerospike_sindex_stage_size_range_duration | default(aerospike_sindex_stage_size_range_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}SIndex stage size configuration is not configured according to documentation in {{ $labels.instance }}/{{ $labels.ns }}{% endraw %}"
      description: "{% raw %}SIndex stage size configuration is not configured according to documentation in {{ $labels.ns }} in node {{ $labels.instance }}. {% endraw %}"

  - alert: AerospikeIndexPressureDirtyMemoryWarn
    # Check here: https://docs.aerospike.com/reference/info#index-pressure
    expr: (((aerospike_namespace_index_pressure_dirty_memory{ job="aerospike", cluster_name="{{ thresholds.cluster }}" })/(aerospike_namespace_index_pressure_total_memory{ job="aerospike", cluster_name="{{ thresholds.cluster }}" })*100)>{{ thresholds.aerospike_index_pressure_dirty_mem_suggested_limit | default(aerospike_index_pressure_dirty_mem_suggested_limit) }}) 
    for: {{ thresholds.aerospike_index_pressure_dirty_mem_duration | default(aerospike_index_pressure_dirty_mem_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Dirty memory ratio against the total memory is above configured limit in node {{ $labels.instance }}{% endraw %}"
      description: "{% raw %}Dirty memory ration against the total memory is above configured limit in node {{ $labels.instance }}{% endraw %}"
      
  - alert: NamespaceDiskCloseToStopWrites
    expr: (aerospike_namespace_device_available_pct{job="aerospike", cluster_name="{{ thresholds.cluster }}"} - aerospike_namespace_storage_engine_min_avail_pct{job="aerospike", cluster_name="{{ thresholds.cluster }}"}) <= 10
    for: {{ thresholds.aerospike_namespace_stop_writes_duration | default(aerospike_namespace_stop_writes_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Close to stop writes for {{ $labels.instance }}/{{ $labels.ns }} due to device_available_pct{% endraw %}"
      description: "{% raw %}device_available_pct for namespace {{ $labels.ns }} in node {{ $labels.instance }} is close to min-avail-pct (stop writes) limit.{% endraw %}"

  - alert: NamespaceMemoryCloseToStopWrites
    expr: (aerospike_namespace_stop_writes_pct{job="aerospike", cluster_name="{{ thresholds.cluster }}"} - (100 - aerospike_namespace_memory_free_pct{job="aerospike", cluster_name="{{ thresholds.cluster }}"})) <= 10
    for: {{ thresholds.aerospike_namespace_stop_writes_duration | default(aerospike_namespace_stop_writes_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Close to stop writes for {{ $labels.instance }}/{{ $labels.ns }} due to memory {% endraw %}"
      description: "{% raw %}Free memory for namespace {{ $labels.ns }} in node {{ $labels.instance }} is close to stop writes limit.{% endraw %}"
  
  - alert: NamespacePmemCloseToStopWrites
    expr: (aerospike_namespace_pmem_available_pct{job="aerospike", cluster_name="{{ thresholds.cluster }}"} - aerospike_namespace_storage_engine_min_avail_pct{job="aerospike", cluster_name="{{ thresholds.cluster }}"}) <= 10
    for: {{ thresholds.aerospike_namespace_stop_writes_duration | default(aerospike_namespace_stop_writes_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Close to stop writes for {{ $labels.instance }}/{{ $labels.ns }} due to pmem_available_pct{% endraw %}"
      description: "{% raw %}pmem_available_pct for namespace {{ $labels.ns }} in node {{ $labels.instance }} is close to min-avail-pct (stop writes) limit.{% endraw %}"
  
  - alert: NamespaceFreeMemoryCloseToStopWrites
    expr: (aerospike_namespace_stop_writes_sys_memory_pct{job="aerospike", cluster_name="{{ thresholds.cluster }}"} - scalar(100 - (aerospike_node_stats_system_free_mem_pct{job="aerospike", cluster_name="{{ thresholds.cluster }}"}))) <= 10
    for: {{ thresholds.aerospike_namespace_stop_writes_duration | default(aerospike_namespace_stop_writes_duration) }}
    labels:
      severity: critical
    annotations:
      summary: "{% raw %}Close to stop writes for {{ $labels.instance }}/{{ $labels.ns }} due to memory{% endraw %}"
      description: "{% raw %}Free memory for namespace {{ $labels.ns }} in node {{ $labels.instance }} is close to stop writes limit.{% endraw %}"

  - alert: ActiveProxies
    expr: (increase(aerospike_namespace_client_proxy_complete{job="aerospike", cluster_name="{{ thresholds.cluster }}"}[2m]) + increase(aerospike_namespace_client_proxy_timeout{job="aerospike", cluster_name="{{ thresholds.cluster }}"}[2m]) + increase(aerospike_namespace_client_proxy_error{job="aerospike", cluster_name="{{ thresholds.cluster }}"}[2m]) + increase(aerospike_namespace_batch_sub_proxy_complete{job="aerospike", cluster_name="{{ thresholds.cluster }}"}[2m]) + increase(aerospike_namespace_batch_sub_proxy_timeout{job="aerospike", cluster_name="{{ thresholds.cluster }}"}[2m]) + increase(aerospike_namespace_batch_sub_proxy_error{job="aerospike", cluster_name="{{ thresholds.cluster }}"}[2m])) > 0
    for: {{ thresholds.aerospike_active_proxies_duration | default(aerospike_active_proxies_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Node is doing proxy. Proxies can happen during cluster change / migrations or if there are any network issues.{% endraw %}"
      description: "{% raw %}Active proxies detected for {{ $labels.ns }} on node {{ $labels.instance }}{% endraw %}"

  - alert: NamespaceSupervisorFallingBehind
    expr: aerospike_namespace_nsup_cycle_deleted_pct{job="aerospike", cluster_name="{{ thresholds.cluster }}"} > 1 # (Aerospike 6.3 and later) 
    for: {{ thresholds.aerospike_nsup_fall_behind_duration | default(aerospike_nsup_fall_behind_duration) }}
    labels:
      severity: critical
    annotations:
      summary: "{% raw %}NSUP is falling behind and/or display the length of time the most recent NSUP cycle lasted{% endraw %}"
      description: "{% raw %}There seems some lag falling behind and/or display the length of time the most recent NSUP cycle lasted {{ $labels.ns }} in node {{ $labels.instance }}{% endraw %}"

  - alert: HwmBreached
    expr: aerospike_namespace_hwm_breached{job="aerospike", cluster_name="{{ thresholds.cluster }}"} == 1
    for: {{ thresholds.aerospike_hwm_breached_duration | default(aerospike_hwm_breached_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}High water mark breached for {{ $labels.instance }}/{{ $labels.ns }}{% endraw %}"
      description: "{% raw %}high-water-disk-pct or high-water-memory-pct has been breached for namespace {{ $labels.ns }} in node {{ $labels.instance }}. Eviction may start to recover disk space.{% endraw %}"

  - alert: LowDeviceAvailWarning
    expr: aerospike_namespace_device_available_pct{job="aerospike", cluster_name="{{ thresholds.cluster }}"} < {{ thresholds.low_device_warning | default(aerospike_low_device_avail_warning_minimum_pct) }}
    for: {{ thresholds.aerospike_low_device_avail_warning_duration | default(aerospike_low_device_avail_warning_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Device available warning for {{ $labels.instance }}/{{ $labels.ns }}{% endraw %}"
      description: "Device available has dropped below {{ thresholds.low_device_warning | default(aerospike_low_device_avail_warning_minimum_pct) }}%{% raw %} for namespace {{ $labels.ns }} in node {{ $labels.instance }}. May indicate that defrag is unable to keep up with the current load, and may result in stop writes if it continues to drop.{% endraw %}"

  - alert: LowDeviceAvailCritical
    expr: aerospike_namespace_device_available_pct{job="aerospike", cluster_name="{{ thresholds.cluster }}"} < {{ thresholds.low_device_critical | default(aerospike_low_device_avail_critical_minimum_pct) }}
    for: {{ thresholds.aerospike_low_device_avail_critical_duration | default(aerospike_low_device_avail_critical_duration) }}
    labels:
      severity: critical
    annotations:
      summary: "{% raw %}Device available critically low for {{ $labels.instance }}/{{ $labels.ns }}{% endraw %}"
      description: "Device available has dropped below {{ thresholds.low_device_critical | default(aerospike_low_device_avail_critical_minimum_pct) }}%{% raw %} for namespace {{ $labels.ns }} in node {{ $labels.instance }}. May indicate that defrag is unable to keep up with the current load, and may result in stop writes if it continues to drop.{% endraw %}"

  - alert: ClientTimeouts
    expr: rate(aerospike_namespace_client_read_timeout{job="aerospike", cluster_name=~"{{ thresholds.cluster }}"}[{{ aerospike_namespace_client_read_timeout_range }}]) > {{ thresholds.client_timeouts | default(client_timeouts) }} or rate(aerospike_namespace_client_write_timeout{job="aerospike", cluster_name=~"{{ thresholds.cluster }}"}[{{ aerospike_namespace_client_write_timeout_range }}]) > {{ thresholds.client_timeouts | default(client_timeouts) }} or rate(aerospike_namespace_client_tsvc_timeout{job="aerospike", cluster_name=~"{{ thresholds.cluster }}"}[{{ aerospike_namespace_client_tsvc_timeout_range }}]) > {{ thresholds.client_timeouts | default(client_timeouts) }}
    for: {{ thresholds.client_timeout_duration | default(aerospike_client_timeout_duration) }}
    labels:
      severity: critical
    annotations:
      summary: "Client transactions are timing out"
      description: "Client connections timing out at a rate greater than {{ thresholds.client_timeouts | default(client_timeouts) }}/s. Timeouts can occur during network issues or resource contention on the client and/or server nodes."

  - alert: LowMemoryNamespaceWarning
    expr: aerospike_namespace_memory_free_pct{job="aerospike", cluster_name="{{ thresholds.cluster }}"} < {{ thresholds.low_memory_ns_warning | default(aerospike_low_memory_namespace_warning_minimum_pct) }}
    for: {{ thresholds.aerospike_low_memory_namespace_warning_duration | default(aerospike_low_memory_namespace_warning_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Memory available warning for {{ $labels.instance }}/{{ $labels.ns }}{% endraw %}"
      description: "Memory free has dropped below {{ thresholds.low_memory_ns_warning | default(aerospike_low_memory_namespace_warning_minimum_pct) }}%{% raw %} for namespace {{ $labels.ns }} in node {{ $labels.instance }}. May indicate a need to reduce the object count or increase capacity.{% endraw %}"

  - alert: LowMemoryNamespaceCritical
    expr: aerospike_namespace_memory_free_pct{job="aerospike", cluster_name="{{ thresholds.cluster }}"} < {{ thresholds.low_memory_ns_critical | default(aerospike_low_memory_namespace_critical_minimum_pct) }}
    for: {{ thresholds.aerospike_low_memory_namespace_critical_duration | default(aerospike_low_memory_namespace_critical_duration) }}
    labels:
      severity: critical
    annotations:
      summary: "{% raw %}Memory available critically low for {{ $labels.instance }}/{{ $labels.ns }}{% endraw %}"
      description: "Memory free has dropped below {{ thresholds.low_memory_ns_critical | default(aerospike_low_memory_namespace_critical_minimum_pct) }}%{% raw %} for namespace {{ $labels.ns }} in node {{ $labels.instance }}. May indicate a need to reduce the object count or increase capacity.{% endraw %}"

  - alert: DeviceWriteQWarning
    expr: aerospike_namespace_storage_engine_device_write_q{job="aerospike", cluster_name="{{ thresholds.cluster }}"} > {{ thresholds.device_write_q_warning | default(aerospike_device_write_q_warning_minimum_size) }}
    for: {{ thresholds.aerospike_device_write_q_warning_duration | default(aerospike_device_write_q_warning_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Device write queue high for {{ $labels.instance }}/{{ $labels.ns }}/{{ $labels.device_index }}{% endraw %}"
      description: "Device write queue is greater than {{ thresholds.device_write_q_warning | default(aerospike_device_write_q_warning_minimum_size) }}{%raw%} for namespace {{ $labels.ns }} on device {{ $labels.device_index }} in node {{ $labels.instance }}. May indicate underperforming storage subsystem or hotkeys.{% endraw %}"

  - alert: ShadowDeviceWriteQWarning
    expr: aerospike_namespace_storage_engine_device_shadow_write_q{job="aerospike", cluster_name="{{ thresholds.cluster }}"} > {{ thresholds.device_shadow_write_q_warning | default(aerospike_shadow_device_write_q_warning_minimum_size) }}
    for: {{ thresholds.aerospike_shadow_device_write_q_warning_duration | default(aerospike_shadow_device_write_q_warning_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Shadow device write queue high for {{ $labels.instance }}/{{ $labels.ns }}/{{ $labels.device_index }}{% endraw %}"
      description: "Shadow device write queue is greater than {{ thresholds.device_shadow_write_q_warning | default(aerospike_shadow_device_write_q_warning_minimum_size) }} {% raw %}for namespace {{ $labels.ns }} on device {{ $labels.device_index }} in node {{ $labels.instance }}. May indicate underperforming storage subsystem or hotkeys.{% endraw %}"

  - alert: DeviceDefragQWarning
    expr: aerospike_namespace_storage_engine_device_defrag_q{job="aerospike", cluster_name="{{ thresholds.cluster }}"}> {{ thresholds.device_defrag_q_warning | default(aerospike_device_defrag_q_breach_size) }}
    for: {{ thresholds.device_defrag_q_breach_duration | default(aerospike_device_defrag_q_breach_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Device defrag queue high for {{ $labels.instance }}/{{ $labels.ns }}/{{ $labels.device_index }}{% endraw %}"
      description: "Device defrag queue has been above {{ thresholds.device_defrag_q_warning | default(aerospike_device_defrag_q_breach_size) }} for more than {{ thresholds.device_defrag_q_breach_duration | default('5m') }}{% raw %} for namespace {{ $labels.ns }} on device {{ $labels.device_index }} in node {{ $labels.instance }}. May indicate underperforming storage subsystem or hotkeys.{% endraw %}"

  - alert: ClockSkewStopWrites
    expr: aerospike_namespace_clock_skew_stop_writes{job="aerospike", cluster_name="{{ thresholds.cluster }}"} == 1
    for: {{ thresholds.aerospike_clock_skew_stop_writes_duration | default(aerospike_clock_skew_stop_writes_duration) }}
    labels:
      severity: critical
    annotations:
      summary: "Clock skew stop writes"
      description: "{% raw %}Clock has skewed for namespace {{ $labels.ns }} in node {{ $labels.instance }}{% endraw %}"

  - alert: UnavailablePartitions
    expr: aerospike_namespace_unavailable_partitions{job="aerospike", cluster_name="{{ thresholds.cluster }}"} > {{ thresholds.unavailable_partitions | default(aerospike_unavailable_partitions_minimum) }}
    for: {{ thresholds.aerospike_unavailable_partitions_duration | default(aerospike_unavailable_partitions_duration) }}
    labels:
      severity: critical
    annotations:
      summary: "Some partitions are inaccessible, and roster nodes are missing from the cluster."
      description: "{% raw %}Some partitions are not available for namespace {{ $labels.ns }} on node {{ $labels.instance }}. Check for network issues and make sure the cluster forms properly.{% endraw %}"

  - alert: DeadPartitions
    expr: aerospike_namespace_dead_partitions{job="aerospike", cluster_name="{{ thresholds.cluster }}"} > {{ thresholds.dead_partitions | default(aerospike_dead_partitions_minimum) }}
    for: {{ thresholds.aerospike_dead_partitions_duration | default(aerospike_dead_partitions_duration) }}
    labels:
      severity: critical
    annotations:
      summary: "There are unavailable partition, but all roster nodes are present in the cluster."
      description: "{% raw %}Some partitions are dead for namespace {{ $labels.ns }} on node {{ $labels.instance }}. Greater than replication-factor number nodes had an unclean shutdown, and there may be data loss. Will require the use of the revive command to make the partitions available again.{% endraw %}"
  
- name: aerospike_{{ thresholds.cluster }}.rules > NODE
  rules:
  - alert: PrometheusNodeExporterNotPresent
    expr: absent(node_cpu_seconds_total) == 1
    for: 30s
    labels:
      severity: warn
    annotations:
      summary: "{% raw %} Prometheus Node Exporter is not configured {% endraw %}"
      description: "{% raw %} Prometheus Node Exporter is not configured in {{ $labels.instance }} {% endraw %}"

  - alert: BestPracticesFailure
    expr: aerospike_node_stats_failed_best_practices{job="aerospike", cluster_name="{{ thresholds.cluster }}"} > 0
    for: {{ thresholds.aerospike_node_stats_failed_best_practices_warn_duration | default(aerospike_node_stats_failed_best_practices_warn_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %} Best Practices check failed on {{ $labels.instance }} in cluster {{ $labels.cluster_name }}{% endraw %}"
      description: "{% raw %} Best Practices check failed on {{ $labels.instance }} in cluster {{ $labels.cluster_name }}{% endraw %}"

  - alert: ClusterSize
    expr: aerospike_node_stats_cluster_size{job="aerospike", cluster_name="{{ thresholds.cluster }}"} < {{ thresholds.cluster_size | default(aerospike_cluster_sze_minimum) }}
    for: {{ thresholds.aerospike_cluster_size_duration | default(aerospike_cluster_size_duration) }}
    labels:
      severity: critical
    annotations:
      summary: "Cluster size lower than expected"
      description: "{% raw %}Cluster size mismatch for node {{ $labels.instance }}{% endraw %}"

  - alert: ClientConnectionsWarning
    expr: aerospike_node_stats_client_connections{job="aerospike", cluster_name="{{ thresholds.cluster }}"} > {{ thresholds.client_connections_warning | default(aerospike_client_connections_warning_level) }}
    for: {{ thresholds.aerospike_client_connections_warning_duration | default(aerospike_client_connections_warning_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "Client connections warning"
      description: "Client connections are greater than {{ thresholds.client_connections_warning | default(aerospike_client_connections_warning_level) }}. Connections will fail if they exceed proto-fd-max."
  - alert: ClientConnectionsCritical
    expr: aerospike_node_stats_client_connections{job="aerospike", cluster_name="{{ thresholds.cluster }}"} > {{ thresholds.client_connections_critical | default(aerospike_client_connections_critical_level) }}
    for: {{ thresholds.aerospike_client_connections_critical_duration | default(aerospike_client_connections_critical_duration) }}
    labels:
      severity: critical
    annotations:
      summary: "Client connections warning"
      description: "Client connections are greater than expected peak of {{ thresholds.client_connections_critical | default(aerospike_client_connections_critical_level) }}."

  - alert: ClientConnectionChurn
    expr: rate(aerospike_node_stats_client_connections_opened{job="aerospike", cluster_name="{{ thresholds.cluster }}"}[{{ aerospike_node_stats_client_connections_opened_range }}]) > {{ thresholds.client_connection_churn | default(aerospike_client_connection_churn_level) }} or rate(aerospike_node_stats_client_connections_closed{job="aerospike", cluster_name="{{ thresholds.cluster }}"}[{{ aerospike_node_stats_client_connections_closed_range }}]) > {{ thresholds.client_connection_churn | default(aerospike_client_connection_churn_level) }}
    for: {{ thresholds.client_connection_churn_duration | default(aerospike_client_connection_churn_duration) }}
    labels:
      severity: critical
    annotations:
      summary: "Clients are churning connections at a high rate"
      description: "Client connections are being opened or closed at a rate greater than {{ thresholds.client_connection_churn | default(aerospike_client_connection_churn_level) }}/s. Connection churn can increase latency and client timeouts which in turn cause the client to open more connections."

  - alert: ClockSkewWarning
    expr: aerospike_node_stats_cluster_clock_skew_ms{job="aerospike", cluster_name="{{ thresholds.cluster }}"} > {{ thresholds.clock_skew_warning | default(aerospike_clock_skew_warning_maximum_ms) }}
    for: {{ thresholds.aerospike_clock_skew_warning_duration | default(aerospike_clock_skew_warning_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "Cluster clock skew warning{"
      description: "Current maximum clock skew between nodes - will trigger stop writes when it exceeds {{ thresholds.clock_skew_warning | default(aerospike_clock_skew_warning_maximum_ms) }} seconds if nsup-period is non-zero."

  - alert: ClockSkewCritical
    expr: aerospike_node_stats_cluster_clock_skew_ms{job="aerospike", cluster_name="{{ thresholds.cluster }}"} > {{ thresholds.clock_skew_critical | default(aerospike_clock_skew_critical_maximum_ms) }}
    for: {{ thresholds.aerospike_clock_skew_critical_duration | default(aerospike_clock_skew_critical_duration) }}
    labels:
      severity: critical
    annotations:
      summary: "Cluster clock skew critical alert"
      description: "Current maximum clock skew between nodes - will trigger stop writes when it exceeds {{ thresholds.clock_skew_critical | default(aerospike_clock_skew_critical_maximum_ms) }} if nsup-period is non-zero."

  - alert: LowMemorySystemWarning
    expr: aerospike_node_stats_system_free_mem_pct{job="aerospike", cluster_name="{{ thresholds.cluster }}"} < {{ thresholds.low_memory_system_warning | default(aerospike_low_memory_system_warning_minimum_pct) }}
    for: {{ thresholds.aerospike_low_memory_system_warning_duration | default(aerospike_low_memory_system_warning_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Memory available warning for {{ $labels.instance }}{% endraw %}"
      description: "Total memory free has dropped below {{ thresholds.low_memory_system_warning | default(aerospike_low_memory_system_warning_minimum_pct) }}%{% raw %} for node {{ $labels.instance }}.{% endraw %}"

  - alert: LowMemorySystemCritical
    expr: aerospike_node_stats_system_free_mem_pct{job="aerospike", cluster_name="{{ thresholds.cluster }}"} < {{ thresholds.low_memory_system_critical | default(aerospike_low_memory_system_critical_minimum_pct) }}
    for: {{ thresholds.aerospike_low_memory_system_critical_duration | default(aerospike_low_memory_system_critical_duration) }}
    labels:
      severity: critical
    annotations:
      summary: "{% raw %}Memory available critically low for {{ $labels.instance }}{% endraw %}"
      description: "Total memory free has dropped below {{ thresholds.low_memory_system_critical | default(aerospike_low_memory_system_critical_minimum_pct) }}%{% raw %} for node {{ $labels.instance }}.{% endraw %}"

  - alert: HeapEfficiencyWarning
    #expr: aerospike_node_stats_heap_efficiency_pct{job="aerospike", cluster_name="{{ thresholds.cluster }}"} < {{ thresholds.heap_warning | default(60) }}
    expr: (100 - aerospike_node_stats_system_free_mem_pct{job="aerospike", cluster_name="{{ thresholds.cluster }}"}) > {{ thresholds.heap_warning_condition | default(aerospike_heap_efficiency_warning_used_mem_pct) }} and aerospike_node_stats_heap_efficiency_pct{job="aerospike", cluster_name="{{ thresholds.cluster }}"} < {{ thresholds.heap_warning | default(aerospike_heap_efficiency_warning_maximum_pct) }}
    for: {{ thresholds.aerospike_heap_efficiency_warning_duration | default(aerospike_heap_efficiency_warning_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Heap efficiency warning for {{ $labels.instance }}{% endraw %}"
      description: "{% raw %}Heap efficiency for node for {{ $labels.instance }} has dropped below {% endraw %}{{ thresholds.heap_warning | default(aerospike_heap_efficiency_warning_maximum_pct) }}%."

  - alert: RwInProgressWarning
    expr: aerospike_node_stats_rw_in_progress{job="aerospike", cluster_name="{{ thresholds.cluster }}"}> {{ thresholds.rw_in_progress_warning | default(aerospike_rw_in_progress_warning_q_size) }}
    for: {{ thresholds.aerospike_rw_in_progress_warning_duration | default(aerospike_rw_in_progress_warning_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Read/write queue too high for {{ $labels.instance }}/{{ $labels.ns }}/{{ $labels.device_index }}{% endraw %}"
      description: "Read/write queue is greater than {{ thresholds.rw_in_progress_warning | default(aerospike_rw_in_progress_warning_q_size) }}{% raw %} for namespace {{ $labels.ns }} on device {{ $labels.device_index }} in node {{ $labels.instance }}. May indicate underperforming storage subsystem or hotkeys.{% endraw %}"

- name: aerospike_{{ thresholds.cluster }}.rules > SET
  rules:
    - alert: NamespaceSetQuotaWarning
      expr: (((aerospike_sets_device_data_bytes{job="aerospike", cluster_name="{{ thresholds.cluster }}"} + aerospike_sets_memory_data_bytes{job="aerospike", cluster_name="{{ thresholds.cluster }}"}) / (aerospike_sets_stop_writes_size{job="aerospike", cluster_name="{{ thresholds.cluster }}"} != 0)) * 100) > {{ thresholds.aerospike_set_quota_warning_pct | default(aerospike_set_quota_warning_pct) }}
      for: {{ thresholds.aerospike_set_quota_duration | default(aerospike_set_quota_duration) }}
      labels:
        severity: critical
      annotations:
        description: "{% raw %}Nearing memory quota for {{ $labels.set }} in namespace {{ $labels.ns }} in node {{ $labels.instance }}.{% endraw %}"
        summary: "{% raw %}One of your nodes is at{% endraw %} {{ thresholds.aerospike_set_quota_warning_pct }} {% raw %}% of the quota you have configured on the set.{% endraw %}"
  
    - alert: NamespaceSetQuotaAlert
      expr: (((aerospike_sets_device_data_bytes{job="aerospike", cluster_name="{{ thresholds.cluster }}"} + aerospike_sets_memory_data_bytes{job="aerospike", cluster_name="{{ thresholds.cluster }}"}) / (aerospike_sets_stop_writes_size{job="aerospike", cluster_name="{{ thresholds.cluster }}"} != 0)) * 100) > {{ thresholds.aerospike_set_quota_alert_pct | default(aerospike_set_quota_alert_pct) }}
      for: {{ thresholds.aerospike_set_quota_duration | default(aerospike_set_quota_duration) }}
      labels:
        severity: critical
      annotations:
        description: "{% raw %}At or Above memory quota for {{ $labels.set }} in namespace {{ $labels.ns }} in node {{ $labels.instance }}.{% endraw %}"
        summary: "{% raw %}One of your nodes is at{% endraw %} {{ thresholds.aerospike_set_quota_alert_pct }}{% raw %}% of the quota you have configured on the set.{% endraw %}"
  
- name: aerospike_{{ thresholds.cluster }}.rules > LATENCIES
  rules:
  - alert: ReadLatencyP95Warning
    expr: histogram_quantile(0.95, (aerospike_latencies_read_ms_bucket{job="aerospike", cluster_name="{{ thresholds.cluster }}"})) > {{ thresholds.read_latency_p95 | default(2) }}
    for: {{ thresholds.aerospike_read_latency_p95_warning_duration | default(aerospike_read_latency_p95_warning_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Read latency breached for {{ $labels.ns }} on {{ $labels.instance }}{% endraw %}"
      description: "95th percentile read latency breached {{ thresholds.read_latency_p95 | default(2) }}{% raw %}ms for namespace {{ $labels.ns }} on host {{ $labels.instance }}.{% endraw %}"

  - alert: ReadLatencyP99Warning
    expr: histogram_quantile(0.99, (aerospike_latencies_read_ms_bucket{job="aerospike", cluster_name="{{ thresholds.cluster }}"})) > {{ thresholds.read_latency_p99 | default(4) }}
    for: {{ thresholds.aerospike_read_latency_p99_warning_duration | default(aerospike_read_latency_p99_warning_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Read latency breached for {{ $labels.ns }} on {{ $labels.instance}}{% endraw %}"
      description: "99th percentile read latency breached {{ thresholds.read_latency_p99 | default(4) }}{% raw %}ms for namespace {{ $labels.ns }} on host {{ $labels.instance }}.{% endraw %}"

  - alert: ReadLatencyP999Warning
    expr: histogram_quantile(0.999, (aerospike_latencies_read_ms_bucket{job="aerospike", cluster_name="{{ thresholds.cluster }}"})) > {{ thresholds.read_latency_p999 | default(16) }}
    for: {{ thresholds.aerospike_read_latency_p999_warning_duration | default(aerospike_read_latency_p999_warning_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Read latency breached for {{ $labels.ns }} on {{ $labels.instance }}{% endraw %}"
      description: "99.9th percentile read latency breached {{ thresholds.write_latency_p999 | default(16) }}{% raw %}ms for namespace {{ $labels.ns }} on host {{ $labels.instance }}.{% endraw %}"

  - alert: WriteLatencyP95Warning
    expr: histogram_quantile(0.95, (aerospike_latencies_write_ms_bucket{job="aerospike", cluster_name="{{ thresholds.cluster }}"})) > {{ thresholds.write_latency_p95 | default(4) }}
    for: {{ thresholds.aerospike_write_latency_p95_warning_duration | default(aerospike_write_latency_p95_warning_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Write latency breached for {{ $labels.ns }} on {{ $labels.instance }}{% endraw %}"
      description: "95th percentile write latency breached {{ thresholds.write_latency_p95 | default(4) }}{% raw %}ms for namespace {{ $labels.ns }} on host {{ $labels.instance }}.{% endraw %}"

  - alert: WriteLatencyP99Warning
    expr: histogram_quantile(0.99, (aerospike_latencies_write_ms_bucket{job="aerospike", cluster_name="{{ thresholds.cluster }}"})) > {{ thresholds.write_latency_p99 | default(16) }}
    for: {{ thresholds.aerospike_write_latency_p99_warning_duration | default(aerospike_write_latency_p99_warning_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Write latency breached for {{ $labels.ns }} on {{ $labels.instance }}{% endraw %}"
      description: "99th percentile write latency breached {{ thresholds.write_latency_p99 | default(16) }}{% raw %}ms for namespace {{ $labels.ns }} on host {{ $labels.instance }}.{% endraw %}"

  - alert: WriteLatencyP999Warning
    expr: histogram_quantile(0.999, (aerospike_latencies_write_ms_bucket{job="aerospike", cluster_name="{{ thresholds.cluster }}"})) > {{ thresholds.write_latency_p999 | default(64) }}
    for: {{ thresholds.aerospike_write_latency_p999_warning_duration | default(aerospike_write_latency_p999_warning_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Write latency breached for {{ $labels.ns }} on {{ $labels.instance }}{% endraw %}"
      description: "99.9th percentile write latency breached {{ thresholds.write_latency_p999 | default(64) }}{% raw %}ms for namespace {{ $labels.ns }} on host {{ $labels.instance }}.{% endraw %}"

{% if thresholds.xdr_dcs is defined %}
- name: aerospike_{{ thresholds.cluster }}.rules > XDR
  rules:
{% for xdr_dcs in thresholds.xdr_dcs %}
  - alert: XDRTimelag
    expr: aerospike_xdr_lag{job="aerospike", cluster_name="{{ thresholds.cluster }}", dc="{{ xdr_dcs.dc }}"} > {{ xdr_dcs.xdr_lag | default(aerospike_xdr_lag_minimum) }}
    for: {{ xdr_dcs.aerospike_xdr_time_lag_duration | default(aerospike_xdr_time_lag_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}XDR lag for namespace {{ $labels.ns }} exceeding {% endraw %}{{ xdr_dcs.xdr_lag | default(aerospike_xdr_lag_minimum) }}{% raw %} second(s) from node {{ $labels.instance }} to DC {{ $labels.dc }}{% endraw %}"
      description: "XDR lag may be due to network connectivity issues, inability for the source to keep up with incoming writes, or write failures at the destination."
  - alert: XDRAbandonedRecords
    expr: rate(aerospike_xdr_abandoned{job="aerospike", cluster_name="{{ thresholds.cluster }}", dc="{{ xdr_dcs.dc }}"}[{{ aerospike_xdr_abandoned_records_range }}]) > 0
    for: {{ xdr_dcs.aerospike_xdr_abandoned_records_duration | default(aerospike_xdr_abandoned_records_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}Abandoned records detected for XDR on node {{ $labels.instance }} to DC {{ $labels.dc }}{% endraw %}"
      description: "{% raw %}Records abandoned at a destination cluster may indicate a configuration mismatch for the namespace between source and destination.{% endraw %}"
  - alert: XDRRetryNoNode
    expr: rate(aerospike_xdr_retry_no_node{job="aerospike", cluster_name="{{ thresholds.cluster }}", dc="{{ xdr_dcs.dc }}"}[{{ aerospike_xdr_retry_no_node_range }}]) > 0
    for: {{ xdr_dcs.aerospike_xdr_retry_no_node_duration | default(aerospike_xdr_retry_no_node_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "{% raw %}XDR retries occuring on node {{ $labels.instance }} to DC {{ $labels.dc }} due to unknown master node destination{% endraw %}"
      description: "XDR cannot determine which destination node is the master."

  - alert: XDRRetryConnReset
    expr: rate(aerospike_xdr_retry_conn_reset{job="aerospike", cluster_name="{{ thresholds.cluster }}", dc="{{ xdr_dcs.dc }}"}[{{ aerospike_xdr_retry_conn_reset_range }}]) > {{ xdr_dcs.xdr_connection_reset_warning | default(aerospike_xdr_retry_conn_reset_maximum_rate_per_sec) }}
    for: {{ xdr_dcs.aerospike_xdr_retry_conn_reset_duration | default(aerospike_xdr_retry_conn_reset_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "Rate of XDR connection resets greater than {{ xdr_dcs.xdr_connection_reset_warning | default(aerospike_xdr_retry_conn_reset_maximum_rate_per_sec) }}{% raw %}/s from {{ $labels.instance }} to DC {{ $labels.dc }} {% endraw %}"
      description: "XDR retries occuring due to due to timeouts, network problems, or destination node restarts."

  - alert: XDRRetryDest
    expr: rate(aerospike_xdr_retry_dest{job="aerospike", cluster_name="{{ thresholds.cluster }}", dc="{{ xdr_dcs.dc }}"}[{{ aerospike_xdr_retry_dest_range }}]) > {{ xdr_dcs.xdr_retry_dest_warning | default(aerospike_xdr_retry_dest_maximum_rate_per_sec) }}
    for: {{ xdr_dcs.aerospike_xdr_retry_dest_duration | default(aerospike_xdr_retry_dest_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "Increase in XDR write retries is greater than {{ xdr_dcs.xdr_retry_dest_warning | default(aerospike_xdr_retry_dest_maximum_rate_per_sec) }}{% raw %}/s from {{ $labels.instance }} to DC {{ $labels.dc }}{% endraw %}"
      description: "XDR retries due to errors returned by the destination node, u.e. key busy or device overload."

  - alert: XDRLatencyWarning
    expr: aerospike_xdr_latency_ms{job="aerospike", cluster_name="{{ thresholds.cluster }}", dc="{{ xdr_dcs.dc }}"} > {{ xdr_dcs.xdr_latency_warning | default(aerospike_xdr_latency_warning_maximum_ms) }}
    for: {{ xdr_dcs.aerospike_xdr_latency_warning_duration | default(aerospike_xdr_latency_warning_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "XDR latency above {{ xdr_dcs.xdr_latency_warning | default(aerospike_xdr_latency_warning_maximum_ms) }}{% raw %}ms from {{ $labels.instance }} to DC {{ $labels.dc }}{% endraw %}"
      description: "Network latency between XDR source and destination over the last {{ aerospike_xdr_latency_warning_duration }} is higher than expected."

  - alert: XDRLap
    expr: aerospike_xdr_lap_us{job="aerospike", cluster_name="{{ thresholds.cluster }}", dc="{{ xdr_dcs.dc }}"} > {{ xdr_dcs.xdr_lap_us | default(aerospike_xdr_lap_maximum_usec) }}
    for: {{ xdr_dcs.aerospike_xdr_lap_duration | default(aerospike_xdr_lap_duration) }}
    labels:
      severity: warn
    annotations:
      summary: "XDR lap time greater than {{ xdr_dcs.xdr_lap_us | default(aerospike_xdr_lap_maximum_usec) }}{% raw %} microseconds from {{ $labels.instance }} to DC {{ $labels.dc }}{% endraw %}"
      description: "The XDR processing cycle time (lap_us) is approaching the configured period-ms value."

  - alert: XDRRecoveries
    expr: increase(aerospike_xdr_recoveries{job="aerospike", cluster_name="{{ thresholds.cluster }}", dc="{{ xdr_dcs.dc }}"}[{{ aerospike_xdr_recoveries_range }}]) > 0
    for:  {{ xdr_dcs.xdr_recoveries_duration | default(aerospike_xdr_recoveries_duration) }}
    labels:
      severity: critical
    annotations:
      summary: "{% raw %}XDR recoveries increasing on {{ $labels.instance }} to DC {{ $labels.dc }}{% endraw %}"
      description: "XDR recoveries happen during reind or may indicate that the in-memory transaction queue is full (the transaction-queue-limit may be too small)."

{% endfor %}
{% endif %}
{% endfor %}
